#!/usr/bin/env python3
"""
Cached Crawler Example
cache_managerë¥¼ í™œìš©í•œ í¬ë¡¤ëŸ¬ ìºì‹œ í†µí•© ì˜ˆì œ
"""

import sys
import os
from pathlib import Path

# script ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from cache_manager import CacheManager
import requests
from bs4 import BeautifulSoup
import json


class CachedWebCrawler:
    """ìºì‹œë¥¼ í™œìš©í•œ ì›¹ í¬ë¡¤ëŸ¬ ì˜ˆì œ"""

    def __init__(self, cache_ttl_days: int = 7):
        self.cache = CacheManager(
            cache_dir=".claude/cache",
            default_ttl_days=cache_ttl_days
        )
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def fetch_with_cache(self, url: str, force_refresh: bool = False) -> dict:
        """
        ìºì‹œë¥¼ í™œìš©í•œ ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°

        Args:
            url: ê°€ì ¸ì˜¬ URL
            force_refresh: ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸°

        Returns:
            í˜ì´ì§€ ë°ì´í„° (title, content, url)
        """
        # 1. ìºì‹œ í™•ì¸ (force_refreshê°€ ì•„ë‹ ë•Œ)
        if not force_refresh:
            cached = self.cache.get(url)
            if cached:
                print(f"âœ… Cache hit: {url}")
                return cached

        print(f"ğŸŒ Fetching from web: {url}")

        # 2. ì›¹ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ
            title = soup.title.string if soup.title else ""

            # main íƒœê·¸ ì°¾ê¸°
            main_content = soup.find('main') or soup.find('article') or soup.find('body')

            # ë…¸ì´ì¦ˆ ì œê±°
            for tag in main_content.select('nav, aside, footer, script, style'):
                tag.decompose()

            content = main_content.get_text(separator='\n', strip=True) if main_content else ""

            data = {
                "url": url,
                "title": title,
                "content": content[:5000],  # ì²˜ìŒ 5000ìë§Œ (í† í° ì ˆì•½)
                "fetched_from": "web"
            }

            # 3. ìºì‹œ ì €ì¥
            self.cache.set(url, data)
            print(f"ğŸ’¾ Cached: {url}")

            return data

        except Exception as e:
            print(f"âŒ Error fetching {url}: {e}")
            return {
                "url": url,
                "title": "Error",
                "content": f"Failed to fetch: {str(e)}",
                "fetched_from": "error"
            }

    def batch_fetch(self, urls: list, force_refresh: bool = False) -> list:
        """
        ì—¬ëŸ¬ URLì„ ë°°ì¹˜ë¡œ ê°€ì ¸ì˜¤ê¸°

        Args:
            urls: URL ë¦¬ìŠ¤íŠ¸
            force_refresh: ìºì‹œ ë¬´ì‹œ

        Returns:
            ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        results = []

        print(f"ğŸ“š Batch fetching {len(urls)} URLs...\n")

        for i, url in enumerate(urls, 1):
            print(f"[{i}/{len(urls)}] {url}")
            data = self.fetch_with_cache(url, force_refresh)
            results.append(data)
            print()

        return results

    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return self.cache.get_stats()

    def clear_cache(self) -> int:
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        return self.cache.clear_all()

    def clear_expired_cache(self) -> int:
        """ë§Œë£Œëœ ìºì‹œë§Œ ì‚­ì œ"""
        return self.cache.clear_expired()


def integrate_with_existing_crawler():
    """ê¸°ì¡´ í¬ë¡¤ëŸ¬ì— ìºì‹œ í†µí•©í•˜ëŠ” ë°©ë²• ì˜ˆì‹œ"""

    print("=" * 80)
    print("ê¸°ì¡´ í¬ë¡¤ëŸ¬ì— ìºì‹œ í†µí•© ê°€ì´ë“œ")
    print("=" * 80)

    guide = """
# ê¸°ì¡´ í¬ë¡¤ëŸ¬ì— ìºì‹œ í†µí•©í•˜ê¸°

## 1. CacheManager import
```python
from cache_manager import CacheManager
```

## 2. í¬ë¡¤ëŸ¬ __init__ì— ìºì‹œ ë§¤ë‹ˆì € ì¶”ê°€
```python
class ClaudeCodeCrawler:
    def __init__(self, ...):
        # ê¸°ì¡´ ì½”ë“œ
        ...

        # ìºì‹œ ë§¤ë‹ˆì € ì¶”ê°€
        self.cache = CacheManager(
            cache_dir=".claude/cache",
            default_ttl_days=7  # ê¸°ìˆ  ë¬¸ì„œëŠ” 7ì¼
        )
```

## 3. get_page_content ë©”ì„œë“œ ìˆ˜ì •
```python
def get_page_content(self, url):
    # ìºì‹œ í™•ì¸
    cached = self.cache.get(url)
    if cached:
        print(f"âœ… Cache hit: {url}")
        return cached.get("html_content")

    # ìºì‹œ ë¯¸ìŠ¤ - ì›¹ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    try:
        response = requests.get(url, headers=self.headers, timeout=15)
        response.raise_for_status()
        html_content = response.text

        # ìºì‹œ ì €ì¥
        self.cache.set(url, {"html_content": html_content})

        return html_content
    except Exception as e:
        print(f"âŒ Error fetching {url}: {str(e)}")
        return None
```

## 4. ìºì‹œ í†µê³„ ì¶œë ¥ (ì„ íƒ)
```python
def crawl(self, start_url, max_pages=50):
    # ê¸°ì¡´ í¬ë¡¤ë§ ì½”ë“œ
    ...

    # í¬ë¡¤ë§ ì™„ë£Œ í›„ ìºì‹œ í†µê³„ ì¶œë ¥
    stats = self.cache.get_stats()
    print(f"\\nğŸ“Š Cache Statistics:")
    print(f"  Hit rate: {stats['hit_rate']}%")
    print(f"  Cache hits: {stats['hit']}")
    print(f"  Cache misses: {stats['miss']}")
```

## í† í° ì ˆê° íš¨ê³¼

- **ìºì‹œ íˆíŠ¸ ì‹œ**: HTML í¬ë¡¤ë§(5,000í† í°) â†’ ìºì‹œ ì¡°íšŒ(50í† í°) = 99% ì ˆê°
- **ì¤‘ë³µ ë°©ë¬¸**: ë™ì¼ URL ì¬ë°©ë¬¸ ì‹œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ 0, í† í° 0
- **ì†ë„ í–¥ìƒ**: ìºì‹œ ì¡°íšŒëŠ” ë°€ë¦¬ì´ˆ ë‹¨ìœ„ (ì›¹ ìš”ì²­ì€ ì´ˆ ë‹¨ìœ„)

## ìºì‹œ ê´€ë¦¬ ëª…ë ¹ì–´

```bash
# ìºì‹œ í†µê³„ í™•ì¸
python script/cache_manager.py --action stats

# ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
python script/cache_manager.py --action clear-expired

# ì „ì²´ ìºì‹œ ì‚­ì œ
python script/cache_manager.py --action clear-all
```
"""

    print(guide)


def demo():
    """ìºì‹œ í¬ë¡¤ëŸ¬ ë°ëª¨"""
    print("=" * 80)
    print("Cached Crawler Demo")
    print("=" * 80 + "\n")

    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = CachedWebCrawler(cache_ttl_days=7)

    # í…ŒìŠ¤íŠ¸ URLë“¤
    test_urls = [
        "https://code.claude.com/docs/en/overview",
        "https://docs.anthropic.com/en/home",
    ]

    # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ ë¯¸ìŠ¤)
    print("ğŸ”µ First run (cache miss expected):\n")
    results1 = crawler.batch_fetch(test_urls[:1])  # 1ê°œë§Œ í…ŒìŠ¤íŠ¸

    # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ íˆíŠ¸)
    print("\n" + "=" * 80)
    print("ğŸŸ¢ Second run (cache hit expected):\n")
    results2 = crawler.batch_fetch(test_urls[:1])

    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š Cache Statistics:")
    print("=" * 80)
    stats = crawler.get_cache_stats()
    print(json.dumps(stats, indent=2))


def main():
    """CLI ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(description="Cached Crawler Example")
    parser.add_argument("--demo", action="store_true", help="Run demo")
    parser.add_argument("--guide", action="store_true", help="Show integration guide")
    parser.add_argument("--url", help="Fetch single URL")
    parser.add_argument("--urls", nargs="+", help="Fetch multiple URLs")
    parser.add_argument("--force", action="store_true", help="Force refresh (skip cache)")

    args = parser.parse_args()

    if args.guide:
        integrate_with_existing_crawler()
    elif args.demo:
        demo()
    elif args.url:
        crawler = CachedWebCrawler()
        data = crawler.fetch_with_cache(args.url, force_refresh=args.force)
        print(json.dumps(data, indent=2, ensure_ascii=False))
    elif args.urls:
        crawler = CachedWebCrawler()
        results = crawler.batch_fetch(args.urls, force_refresh=args.force)
        print(json.dumps(results, indent=2, ensure_ascii=False))
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
