#!/usr/bin/env python3
"""
Claude Code Documentation Crawler
í¬ë¡¤ë§í•œ ë¬¸ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import html2text
import os
import time
import re
from urllib.parse import urljoin, urlparse, urlunparse
from pathlib import Path
import json

DOCS_OUTPUT_DIR = os.path.join("doc", "claude_code_docs")


class ClaudeCodeCrawler:
    def __init__(
        self,
        base_url="https://code.claude.com",
        output_dir=DOCS_OUTPUT_DIR,
        include_path_prefixes=None,
        exclude_path_patterns=None,
    ):
        self.base_url = base_url
        self.output_dir = output_dir
        if include_path_prefixes is None:
            include_path_prefixes = ["/docs/en/"]
        self.include_path_prefixes = include_path_prefixes
        self.exclude_path_patterns = [
            re.compile(pattern) for pattern in (exclude_path_patterns or [])
        ]
        self.visited_urls = set()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = False
        self.html_converter.body_width = 0  # ì¤„ë°”ê¿ˆ ë°©ì§€
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    def strip_noise(self, content):
        """ë„¤ë¹„ê²Œì´ì…˜/í‘¸í„° ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°"""
        if content is None:
            return
        selectors = [
            'nav',
            'aside',
            'footer',
            'form',
            'button',
            '[role="navigation"]',
            '[role="search"]',
            '[aria-label="Search"]',
            '.sidebar',
            '.toc',
            '.table-of-contents',
            '.breadcrumbs',
            '.search',
            '.navigation',
            '.skip-to-content',
        ]
        for selector in selectors:
            for tag in content.select(selector):
                tag.decompose()
        for header in content.find_all('header'):
            if header.find('h1') is None:
                header.decompose()
        
    def is_valid_doc_url(self, url):
        """ë¬¸ì„œ URLì¸ì§€ í™•ì¸"""
        parsed = urlparse(url)
        path = parsed.path or ""
        return (
            parsed.netloc == "code.claude.com" and
            "/docs/" in path and
            (
                not self.include_path_prefixes
                or any(path.startswith(prefix) for prefix in self.include_path_prefixes)
            ) and
            not path.endswith(('.png', '.jpg', '.jpeg', '.gif', '.css', '.js')) and
            not any(pattern.search(path) for pattern in self.exclude_path_patterns)
        )

    def normalize_url(self, url):
        """ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°"""
        parsed = urlparse(url)
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", "", ""))
    
    def get_page_content(self, url):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ Error fetching {url}: {str(e)}")
            return None
    
    def extract_main_content(self, soup):
        """ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        selectors = [
            'main',
            'article',
            '.docs-content',
            '.documentation',
            '[role="main"]',
            '#main-content'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                return content
        
        # ì°¾ì§€ ëª»í•˜ë©´ body ë°˜í™˜
        return soup.find('body')
    
    def clean_filename(self, url):
        """URLì—ì„œ íŒŒì¼ëª… ìƒì„±"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # /docs/en/ ì œê±°
        path = re.sub(r'^docs/(en|ko)/', '', path)
        
        # ë¹ˆ ê²½ë¡œëŠ” indexë¡œ
        if not path or path == 'docs':
            path = 'index'
        
        # ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
        filename = path.replace('/', '_') + '.md'
        return filename
    
    def html_to_markdown(self, html_content):
        """HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
        try:
            markdown = self.html_converter.handle(str(html_content))
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
            markdown = re.sub(r'\n{3,}', '\n\n', markdown)
            return markdown.strip()
        except Exception as e:
            print(f"âš ï¸  Markdown conversion error: {str(e)}")
            return str(html_content)
    
    def extract_links(self, soup, current_url):
        """í˜ì´ì§€ì—ì„œ ë¬¸ì„œ ë§í¬ ì¶”ì¶œ"""
        links = set()
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urljoin(current_url, href)

            full_url = self.normalize_url(full_url)

            if self.is_valid_doc_url(full_url):
                links.add(full_url)
        
        return links
    
    def save_markdown(self, url, content, title=""):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
        filename = self.clean_filename(url)
        filepath = os.path.join(self.output_dir, filename)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata = f"""---
source: {url}
title: {title}
---

"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(metadata + content)
        
        print(f"âœ… Saved: {filename}")
        return filepath
    
    def crawl_page(self, url):
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        if url in self.visited_urls:
            return set()
        
        print(f"\nğŸ” Crawling: {url}")
        self.visited_urls.add(url)
        
        # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        html_content = self.get_page_content(url)
        if not html_content:
            return set()
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = soup.title.string if soup.title else ""
        
        # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ
        main_content = self.extract_main_content(soup)
        self.strip_noise(main_content)
        
        # ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        markdown_content = self.html_to_markdown(main_content)
        
        # ì €ì¥
        self.save_markdown(url, markdown_content, title)
        
        # ë§í¬ ì¶”ì¶œ
        new_links = self.extract_links(soup, url)
        
        return new_links
    
    def crawl(self, start_url, max_pages=50):
        """ì „ì²´ ë¬¸ì„œ í¬ë¡¤ë§"""
        print(f"ğŸš€ Starting crawl from: {start_url}")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        print(f"ğŸ“„ Max pages: {max_pages}\n")
        
        to_visit = {start_url}
        pages_crawled = 0
        
        while to_visit and pages_crawled < max_pages:
            url = to_visit.pop()
            
            try:
                new_links = self.crawl_page(url)
                
                # ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë§í¬ë§Œ ì¶”ê°€
                for link in new_links:
                    if link not in self.visited_urls:
                        to_visit.add(link)
                
                pages_crawled += 1
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ Error crawling {url}: {str(e)}")
                continue
        
        print(f"\nâœ¨ Crawling complete!")
        print(f"ğŸ“Š Pages crawled: {pages_crawled}")
        print(f"ğŸ“ Files saved in: {self.output_dir}")
        
        # í¬ë¡¤ë§ í†µê³„ ì €ì¥
        stats = {
            "total_pages": pages_crawled,
            "visited_urls": list(self.visited_urls),
            "output_dir": self.output_dir
        }
        
        with open(os.path.join(self.output_dir, '_crawl_stats.json'), 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = ClaudeCodeCrawler(
        base_url="https://code.claude.com",
        output_dir=DOCS_OUTPUT_DIR
    )
    
    # ì‹œì‘ URL
    start_url = "https://code.claude.com/docs/en/overview"
    
    # í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ 50í˜ì´ì§€)
    crawler.crawl(start_url, max_pages=50)


if __name__ == "__main__":
    main()
